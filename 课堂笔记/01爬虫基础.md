# 一、爬虫概述

### 1.爬虫必须知道的要素

**爬虫要遵循网上的爬虫机器人协议，怎样查看——在网址后面加上/robots.txt来查看，可以查到哪些是允许的，哪些是不允许的**

爬虫的基本步骤：找到网址，发起请求，解析页面并获得原始数据，对数据进行处理，保存处理后的数据

- 网址分为http开头，https开头端口分别是80和443
- 

### 2.爬虫的基本介绍

##### 2.1网页布局的介绍

首先要知道按下F12之后网页的布局，一般来说就是从左到右是

元素也就是通常说的前端页面的代码，然后是控制台，再之后就是资源，再之后就是网络（一般检查元素就是在这里面，也就是下图的界面），下图中还有headers那里面也有特殊的几个要记得

- 控制台可以输出东西，但是爬虫一般用到最多的就是network网络那个

- request url——请求的网址，request  method——请求的方法，分为get与post（一般get用于直接得到数据，post用于需要提交数据）
  - get方式请求通过params传递参数，传递参数的时候一种是在网址后面加上？然后以id=1的形式进行传递，多个参数用id来拼接，比如后面的钟离就是携带的参数——https://www.baidu.com/s?wd=钟离
  - post方式请求通过data提交表单
- status code——状态码，一般熟悉的就是404请求错误，200是请求成功，500是服务器出错
  - 扩展：100到199是成功接收请求但仍需要填写剩余请求
  - 200到299请求成功，常用200
  - 400到499客户端错误，500到599服务端错误。
- request headers——请求头，如果要提交参数或者需要i用到验证时就需要填写参数。
  - accept——表示接受的数据类型包括什么
  - cookie——记载用户和服务器之间的信息，多用于模拟登录，之后会有演示。

![image-20220124121030088](C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20220124121030088.png)

- UA：user agent——用户标识，用来区分到底自己是不是爬虫的标识，一般需要伪装，以此来绕过robots协议

##### 2.2编码格式的介绍

不同的网页编码的方式有很多种，所以要得到对应的就要用到解码，解码有三种方式

```python
# 第一种直接修改响应数据的编码方式
response.encoding='utf-8'
# 第二种是先将数据转化为二进制，然后在进行解码
response.content.decode()
# 第三种设置默认编码为utf-8
response.encoding=response.apparent_encoding
```

下面是基本演示的代码，输入要搜索的关键字，通过百度网址进行搜索

```python
import requests  # 导入请求包
keyword = input("请输入百度搜索的内容：")
# 找到请求的网址，可以把问好和keyword去掉，然后通过params={"wd":keyword}
url = "https://www.baidu.com/s?"+keyword
# 添加请求的表示，防止自己是爬虫程序,如果没有user agent则结果报错是下面的内容结果错误
'''
<!DOCTYPE html>
<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus=autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=ç¾åº¦ä¸ä¸ class="bg s_btn" autofocus></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ°é»</a> <a href=https://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å°å¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§é¢</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç»å½</a> </noscript> <script>document.write('<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ '" name="tj_login" class="lb">ç»å½</a>');
                </script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">æ´å¤äº§å</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å³äºç¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç¨ç¾åº¦åå¿è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æè§åé¦</a>&nbsp;äº¬ICPè¯030173å·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>

'''
# 用户标识一定要写的
headers ={
    "user-agent":'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0'
}
 # 用请求包来发起对url网址的请求
response = requests.get(url=url,headers=headers)
response.encoding=response.apparent_encoding
# 将获得的页面的元素打印出来
print(response.text)
```

### 3.爬虫的cookie与session会话维持

- 访问网站有时需要输入账号和密码，一般的话第二次来说不再需要输入账号和密码，是因为你已经存入了cookie数据，但是一般来说会断，而session是维持这个链接的时间比较久的
- cookie会作为一个参数传递，传递参数两个形式都是不同的
  - 第一种，在params中传递cookie参数，但是不加s
  - 第二种直接在requests.get中加入cookies，此时cookie有s

##### 3.1得到登陆之后的cookie

```python
import requests  # 导入请求包
# 将cookie转化为字典的包
from requests.utils import dict_from_cookiejar
# 获得需要访问的登陆的网址,当你的登录界面出来的时候，你就随便输一个账号和密码，然后再从network里面去找有关login的网址
url = "http://ptlogin.4399.com/ptlogin/login.do?v=1"
headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"
}


data = {
    "username":"正确的账号",
    "password":"正确的密码"
}
response = requests.post(url=url,data=data,headers=headers)
response.encoding=response.apparent_encoding
print(dict_from_cookiejar(response.cookies))
```

##### 3.2用cookie来进行登录

```python
import requests  # 导入请求包
# 这是登陆之后的网址，访问时需要有cookie
url = "http://my.4399.com/forums/mtags"
headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"
}

cookie = "Cookie: _gprp_c=""; _4399stats_vid=16249520161838411; _4399tongji_vid=162495325663923; Qnick=; zone_guide_limit=1; Hm_lvt_f1fb60d2559a83c8fa1ee6125a352bd7=1625536272,1625628031,1625824645; Puser=598340295; global_hs=4399.com%7C%7C%7C4399%u751F%u6B7B%"
cookie_dic = {}
# 将所有的等号形式转化为键值对的形式
for i in cookie.split(";"):
    cookie_dic[i.split("=")[0]]=i.split("=")[1]

# 这种传参数的时候就是在cookies上面加上s
response = requests.get(url=url,headers=headers,cookies=cookie_dic)
response.encoding=response.apparent_encoding
print(response.text)


```

##### 3.3使用session来进行使用

session可以进行会话维持以至于下次再次访问需要登陆的网站时，就不用再提交表单了

```python
import requests  # 导入请求包
# 这是需要输入账号和密码才能登陆的网址
login_url = "http://ptlogin.4399.com/ptlogin/login.do?v=1"
# 这是登陆之后的网址，访问时需要有cookie
forums_url = "http://my.4399.com/forums/mtags"
headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"
}
data = {
    "username":"正确的账号",
    "password":"正确的密码"
}
# 实例化session对象
session = requests.session()
# 这种传参数的时候就是在cookies上面加上s
response1 = session.post(url=login_url,headers=headers,data=data)
response2 = session.get(url=forums_url,headers=headers)
response2.encoding=response2.apparent_encoding
print(response2.text)
```

### 4.页面跳转来源——防盗链

- 需要在请求头里面多加入一个referer的跳转来源的网址，这是防盗的一种手段

```python
headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"，
    "referer":"跳转的网址"，
    "cookie":"在这添加需要的cookie，记住没有s"
}
```

### 5.代理

代理分为透明代理和普通代理以及匿名代理，一般的话建议使用高匿代理

推荐的代理网站——讯代理http://www.xdaili.cn/buyproxy，89代理https://www.89ip.cn/

代理的使用——防止同一个IP高频率的网站，每次访问时可以换IP

```python
import requests  # 导入请求包
# 这是需要输入账号和密码才能登陆的网址
login_url = "http://ptlogin.4399.com/ptlogin/login.do?v=1"
headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"
}
data = {
    "username":"",
    "password":""
}
# 使用代理的ip，这样可以防止爬取目标时被封禁
proxies={
    "http":"83.36.58.184:8080"
}
session = requests.session()
# 在get上面添加上proxies
response1 = session.post(url=login_url,headers=headers,data=data,proxies=proxies)
```

### 6.禁止重定向与超时处理

```python
import requests  # 导入请求包
# 可以随便找一个发生重定向的网址
login_url = "https://www.google.com/"
headers = {
    "user-agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"
}
# 禁止重定向,加上超时处理（请求时间，响应时间）
try:
    response = requests.get(url=login_url,headers=headers,timeout=(3,7),allow_redirects=False)
    response.encoding=response.apparent_encoding
    print(response.url,response.request.url)
except requests.exceptions.ConnectionError as e:
    print(e)
```

### 7.retrying的使用

```python
import requests  # 导入请求包
from retrying import retry

# 如果抛出异常则会重复进行访问
# @retry(stop_max_attempt_number=3) # 设置最大访问次数
# @retry(stop_max_delay=3000)  # 设置失败重试的最大访问时间，单位是毫秒
# @retry(wait_fixed=2000)  # 设置失败的间隔时间，单位是毫秒
@retry(wait_random_min=1000,wait_random_max=3000)  # 设置失败的随机间隔时间，单位是毫秒，最大是1秒最小是3秒
def emo():
    print("+===========+")
    raise
if __name__ == '__main__':
    emo()
```

用retrying来解决爬虫访问时的超时问题

```python
import requests  # 导入请求包
from retrying import retry

login_url = "https://www.google.com/"
headers = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36 SE 2.X MetaSr 1.0"
}
@retry(stop_max_attempt_number=3)
def spider_url():
    print("+====+")
    response = requests.get(url=login_url,headers=headers,timeout=(3,7),allow_redirects=False)
    response.encoding=response.apparent_encoding
    print(response.url)
spider_url()
```

# 二、记录一下今天其他的心得

### 1.如何清理anaconda中pkgs文件夹

在终端先输入conda clean -p——清除没有连接的依赖

在输入conda clean -t ——清除安装模块的压缩包